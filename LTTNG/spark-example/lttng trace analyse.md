```bash
pgrep -a lttng-sessiond

${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--verbose \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar 100

lttng view > output-lttng.log 2>&1

./build/mvn -DskipTests clean package -rf :spark-examples_2.12
```

then look at logs: 

The log file is a collection of log messages generated by an Apache Spark application, captured using LTTng (Linux Tracing Toolkit Next Generation). Each log entry contains several pieces of information, including the timestamp, CPU ID, message content, logger name, class name, method name, file name, line number, log level, and thread name. Here are some key observations based on the first few lines:

## Log Level Distribution
Warnings (WARN): 3 entries
Informational (INFO): 533 entries


## Logger Name Counts
The most active loggers are from various Spark components. Here are some of the most frequently occurring logger names:
org.apache.spark.executor.Executor (204 occurrences)
org.apache.spark.scheduler.TaskSetManager (200 occurrences)
org.apache.spark.SparkContext (20 occurrences)
org.sparkproject.jetty.server.handler.ContextHandler (28 occurrences)



## Sample Log Entries
Warning (WARN): A sample warning message is about a hostname resolution issue: "Your hostname, Rezghool resolves to a loopback address: 127.0.1.1; using 192.168.0.58 instead (on interface wlp2s0)" from the logger org.apache.spark.util.Utils.
Informational (INFO): An example of an informational message is "Object Called message from Reza." from the logger org.apache.spark.SparkConf$.

## Analysis
Most log entries are informational, indicating regular operation and status updates of the Spark application.
Warning messages highlight potential issues, like network configuration, which might not be critical but should be reviewed for optimal performance.
Frequent logging by Executor and TaskSetManager indicates active task execution and management, typical in a Spark application processing data.
Jetty server logs suggest that a web server (likely for Spark's UI) is also running and being monitored.


https://www.ibm.com/docs/en/zpas/1.1.0?topic=spark-enabling-history-service

```bash
http://10.200.1.183:18080/jobs/
http://localhost:18080/

spark.eventLog.enabled         true
spark.eventLog.dir             /var/spark/events
spark.history.fs.logDirectory /var/spark/events
./sbin/start-history-server.sh
```

* InternalAccumulator.scala

* log4j for spark streaming application: continuously generating logs. usually run for long  periods of time before facing issues that may cause them  to be showt down.
  size of these logs will keep growing over time and making it really difficult to analyze when they start growing past gigabytes.
  spark uses log4j as a logging facility that can leverage and make it work according to our needs.
*   to get logs from both executors and drivers


In Apache Spark, the architecture includes both Drivers and Executors, which play distinct roles in executing a Spark application:

1. **Driver:**
   - **Role:** The Driver is the central coordinator of a Spark application. It is responsible for converting the user's code into tasks and scheduling these tasks on Executors. It also maintains information about the Spark application and responds to the user's input.
   - **Functionality:** The Driver program runs the main() method of the application and is responsible for various tasks such as:
     - Reading the user's program,
     - Creating a SparkContext,
     - Transforming operations into a DAG (Directed Acyclic Graph),
     - Scheduling jobs and tasks,
     - Distributing tasks to Executors.
   - **Location:** Typically, the Driver runs on a dedicated node in the cluster, but in local mode, it can run on a single machine along with an Executor.

2. **Executor:**
   - **Role:** Executors are worker nodes in the cluster that execute the tasks assigned by the Driver. Each Executor runs multiple tasks in separate threads.
   - **Functionality:** The primary responsibilities of an Executor include:
     - Running the tasks assigned to them,
     - Returning results to the Driver,
     - Providing in-memory storage for RDDs (Resilient Distributed Datasets) that are cached by the user's program (if any),
     - Interacting with the storage systems.
   - **Lifecycle:** Executors typically run for the entire lifetime of a Spark application and are static, meaning once lost, they are not re-spawned during the application runtime (except in dynamic allocation mode).

**Key Differences:**
- **Purpose:** The Driver is for coordination and control flow, while Executors are for executing the actual work and data processing.
- **Number:** Typically, there is one Driver per application and multiple Executors.
- **Location:** The Driver is usually on a separate node, whereas Executors are distributed across many nodes in the cluster.
- **Memory Management:** Executors manage the memory for RDD storage, whereas the Driver manages the job and task scheduling.

Understanding the distinction between Drivers and Executors is crucial in optimizing and debugging Spark applications, as it affects how tasks are distributed and executed across the cluster.


Spark Unable to load native-hadoop library for your platform


## Normal run


```bash

${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--verbose \
--master local[*] \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar 1000



${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--verbose \
--master local[*] \
--executor-memory 512m \
--total-executor-cores 1 \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar 1000

lttng view > output-lttng.log 2>&1


${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.streaming.JavaStatefulNetworkWordCount \
--master local[2] \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar \
localhost 9999


${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.JavaPageRank \
--master local[2] \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar



${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.JavaPageRank \
--master local[2] \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar \
/home/rezghool/research/spark_example/input2.txt 3



${SPARK_HOME}/bin/spark-submit \
  --class org.apache.spark.examples.graphx.AggregateMessagesExample \
  --master local[4] \
/opt/spark/examples/target/original-spark-examples_2.12-3.4.0.jar

```
