```bash

${SPARK_HOME}/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--verbose \
/opt/spark/examples/target/scala-2.12/jars/spark-examples_2.12-3.4.0.jar 100

lttng view > output-lttng.log 2>&1
```

then look at logs: 

The log file is a collection of log messages generated by an Apache Spark application, captured using LTTng (Linux Tracing Toolkit Next Generation). Each log entry contains several pieces of information, including the timestamp, CPU ID, message content, logger name, class name, method name, file name, line number, log level, and thread name. Here are some key observations based on the first few lines:

## Log Level Distribution
Warnings (WARN): 3 entries
Informational (INFO): 533 entries


## Logger Name Counts
The most active loggers are from various Spark components. Here are some of the most frequently occurring logger names:
org.apache.spark.executor.Executor (204 occurrences)
org.apache.spark.scheduler.TaskSetManager (200 occurrences)
org.apache.spark.SparkContext (20 occurrences)
org.sparkproject.jetty.server.handler.ContextHandler (28 occurrences)



## Sample Log Entries
Warning (WARN): A sample warning message is about a hostname resolution issue: "Your hostname, Rezghool resolves to a loopback address: 127.0.1.1; using 192.168.0.58 instead (on interface wlp2s0)" from the logger org.apache.spark.util.Utils.
Informational (INFO): An example of an informational message is "Object Called message from Reza." from the logger org.apache.spark.SparkConf$.

## Analysis
Most log entries are informational, indicating regular operation and status updates of the Spark application.
Warning messages highlight potential issues, like network configuration, which might not be critical but should be reviewed for optimal performance.
Frequent logging by Executor and TaskSetManager indicates active task execution and management, typical in a Spark application processing data.
Jetty server logs suggest that a web server (likely for Spark's UI) is also running and being monitored.


https://www.ibm.com/docs/en/zpas/1.1.0?topic=spark-enabling-history-service

```bash
http://10.200.1.183:18080/jobs/
http://localhost:18080/

spark.eventLog.enabled         true
spark.eventLog.dir             /var/spark/events
spark.history.fs.logDirectory /var/spark/events


/sbin/start-history-server.sh/
```


